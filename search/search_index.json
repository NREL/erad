{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Equitable Resilience Analysis for Distribution Power System (ERAD)","text":""},{"location":"#summary","title":"Summary","text":"<p>Understanding the impact of disaster events on people's ability to access critical service is key to designing appropriate programs to minimize the overall impact. Flooded roads, downed power lines, flooded power substation etc. could impact access to critical servies like electricity, food, health and more. The field of disaster modeling is still evolving and so is our understanding of how these events would impact our critical infrastrctures such power grid, hospitals, groceries, banks etc.</p> <p>ERAD is a free, open-source Python toolkit for computing equity and resilience measures in the face of hazards like earthquakes and flooding. It uses graph database to store data and perform computation at the household level for a variety of critical services that are connected by power distribution network. It uses asset fragility curves, which are functions that relate hazard severity to survival probability for power system assets including cables, transformers, substations, roof-mounted solar panels, etc. recommended in top literature. Programs like undergrounding, microgrid, and electricity backup units for critical infrastructures may all be evaluated using metrics and compared across different neighborhoods to assess their effects on equity and resilience.</p> <p>ERAD is designed to be used by researchers, students, community stakeholders, distribution utilities to understand and possibly evaluate effectiveness of different post disaster programs to improve resilience and equity. It was funded by National Renewable Energy Laboratory (NREL) and made publicy available with open license.</p>"},{"location":"computing_metrics/","title":"Computing metrics","text":"<p>Let's see how you can compute metrics. You can compute these metrics post disaster. Click here to learn more about disaster modeling in ERAD.</p>"},{"location":"computing_metrics/#computing-energy-resilience-score","title":"Computing energy resilience score","text":"<p>Use this snippet to compute energy resilience score. </p> <pre><code>from erad.metrics import metric\nfrom erad.db import neo4j_\n\nneo4j_instance = neo4j_.Neo4J(\n                neo4j_url='bolt://localhost:7687',\n                neo4j_username='neo4j', \n                neo4j_password='neo4j')\n\nmetric.energy_resilience_by_customer(\n    neo4j_instance.driver, \"./energy_resilience.csv\",\n    critical_infras=[\"Grocery\", \"Hospital\",\"Convenience\", \"Shelter\", \"Banking\"] \n    )\n</code></pre>"},{"location":"computing_metrics/#checking-if-customer-is-connected-to-grid","title":"Checking if customer is connected to grid","text":"<pre><code>from erad.metrics import metric\nfrom erad.db import neo4j_\n\nneo4j_instance = neo4j_.Neo4J(\n                neo4j_url='bolt://localhost:7687',\n                neo4j_username='neo4j', \n                neo4j_password='neo4j')\n\nmetric.is_customer_getting_power(\n        neo4j_instance.driver, \"./is_customer_connected.csv\")\n</code></pre>"},{"location":"db_neo4j/","title":"db.neo4j","text":"<p>Module contains class and utility functions to manage interactions with Neo4J database.</p>"},{"location":"db_neo4j/#erad.db.neo4j_.Neo4J","title":"<code>Neo4J</code>","text":"<p>Class for managing interaction with Neo4J database.</p> <p>Attributes:</p> Name Type Description <code>neo4j_url</code> <code>str</code> <p>URL for connecting to Neo4j</p> <code>neo4j_username</code> <code>str</code> <p>Username for Neo4j database</p> <code>neo4j_password</code> <code>str</code> <p>Password for Neo4j database</p> <code>use_env</code> <code>bool</code> <p>True if above info are to be collected from env file.</p> <code>driver</code> <code>GraphDatabase.driver</code> <p>Neo4J driver instance</p> Source code in <code>erad\\db\\neo4j_.py</code> <pre><code>class Neo4J:\n\"\"\"Class for managing interaction with Neo4J database.\n\n    Attributes:\n        neo4j_url (str): URL for connecting to Neo4j\n        neo4j_username (str): Username for Neo4j database\n        neo4j_password (str): Password for Neo4j database\n        use_env (bool): True if above info are to be collected\n            from env file.\n        driver (GraphDatabase.driver): Neo4J driver instance\n    \"\"\"\n\n    def __init__(\n        self,\n        neo4j_url: Union[str, None] = None,\n        neo4j_username: Union[str, None] = None,\n        neo4j_password: Union[str, None] = None,\n    ) -&gt; None:\n\"\"\"Constructor for Neo4J class.\n\n        Args:\n            neo4j_url (str): URL for connecting to Neo4j\n            neo4j_username (str): Username for Neo4j database\n            neo4j_password (str): Password for Neo4j database\n        \"\"\"\n\n        self.neo4j_url = NEO4J_URL if NEO4J_URL else neo4j_url\n        self.neo4j_username = (\n            NEO4J_USERNAME if NEO4J_USERNAME else neo4j_username\n        )\n        self.neo4j_password = (\n            NEO4J_PASSWORD if NEO4J_PASSWORD else neo4j_password\n        )\n\n        connection = Neo4jConnectionModel(\n            neo4j_url=self.neo4j_url,\n            neo4j_username=self.neo4j_username,\n            neo4j_password=self.neo4j_password,\n        )\n\n        self.driver = GraphDatabase.driver(\n            connection.neo4j_url,\n            auth=basic_auth(\n                connection.neo4j_username, connection.neo4j_password\n            ),\n        )\n\n        logger.debug(\n            f\"Connected to {connection.neo4j_url} database successfully\"\n        )\n\n    @staticmethod\n    def rename_labels(label):\n\"\"\"Method to replace the invalid character.\"\"\"\n        invalid_chars = [\"-\", \":\", \"(\", \")\", \".\"]\n        for invalid_char in invalid_chars:\n            if invalid_char in label:\n                label = label.replace(invalid_char, \"__\")\n        return label\n\n    # def add_node(\n    #     self,\n    #     labels: Union[List, None] = None,\n    #     properties: Union[Dict, None] = None,\n    # ) -&gt; None:\n    #     \"\"\"Method to add node to the Neo4j database.\n\n    #     Args:\n    #         labels (Union[List, None]): List of labels to be used for node\n    #         properties (Union[Dict, None]): Properties to be used for the node\n    #     \"\"\"\n    #     if labels is None:\n    #         labels = []\n\n    #     if properties is None:\n    #         properties = {}\n\n    #     labels = \":\".join([self.rename_labels(label) for label in labels])\n    #     cypher_query = \"CREATE (:\" + labels + \" $properties)\"\n\n    #     with self.driver.session() as session:\n    #         session.write_transaction(\n    #             lambda tx: tx.run(cypher_query, properties=properties)\n    #         )\n\n    # def add_relationship(\n    #     self,\n    #     from_node_label: str,\n    #     to_node_label: str,\n    #     from_node: str,\n    #     to_node: str,\n    #     relationship_label: str,\n    #     relationship_properties: Union[Dict, None] = None,\n    # ) -&gt; None:\n    #     \"\"\"Method to create relationship in graph database.\n\n    #     Args:\n    #         from_node_label (str): Node label for from node\n    #         to_node_label (str): Node label for to node\n    #         from_node (str): From node name\n    #         to_node (str): To node name\n    #         relationship_label (str): Relationship label name\n    #         relationship_properties (Union[Dict, None]): Properties to be used\n    #             for relationship\n    #     \"\"\"\n\n    #     if relationship_properties is None:\n    #         relationship_properties = {}\n\n    #     cypher_query = (\n    #         f\"MATCH (a:{from_node_label}),(b:{to_node_label}) WHERE a.name = '{from_node}'\"\n    #         + f\" AND b.name = '{to_node}' CREATE (a)-[:\"\n    #         + self.rename_labels(relationship_label)\n    #         + \" $properties]-&gt;(b)\"\n    #     )\n\n    #     with self.driver.session() as session:\n    #         session.write_transaction(\n    #             lambda tx: tx.run(\n    #                 cypher_query, properties=relationship_properties\n    #             )\n    #         )\n\n    # def read_query(self, cypher_query: str) -&gt; List:\n    #     \"\"\"Executes a Cypher read query.\"\"\"\n\n    #     with self.driver.session() as session:\n    #         result = session.read_transaction(\n    #             lambda tx: tx.run(cypher_query).data()\n    #         )\n\n    #     return result\n\n    def close_driver(self):\n\"\"\"Method to close the driver.\"\"\"\n        self.driver.close()\n</code></pre>"},{"location":"db_neo4j/#erad.db.neo4j_.Neo4J.__init__","title":"<code>__init__(neo4j_url=None, neo4j_username=None, neo4j_password=None)</code>","text":"<p>Constructor for Neo4J class.</p> <p>Parameters:</p> Name Type Description Default <code>neo4j_url</code> <code>str</code> <p>URL for connecting to Neo4j</p> <code>None</code> <code>neo4j_username</code> <code>str</code> <p>Username for Neo4j database</p> <code>None</code> <code>neo4j_password</code> <code>str</code> <p>Password for Neo4j database</p> <code>None</code> Source code in <code>erad\\db\\neo4j_.py</code> <pre><code>def __init__(\n    self,\n    neo4j_url: Union[str, None] = None,\n    neo4j_username: Union[str, None] = None,\n    neo4j_password: Union[str, None] = None,\n) -&gt; None:\n\"\"\"Constructor for Neo4J class.\n\n    Args:\n        neo4j_url (str): URL for connecting to Neo4j\n        neo4j_username (str): Username for Neo4j database\n        neo4j_password (str): Password for Neo4j database\n    \"\"\"\n\n    self.neo4j_url = NEO4J_URL if NEO4J_URL else neo4j_url\n    self.neo4j_username = (\n        NEO4J_USERNAME if NEO4J_USERNAME else neo4j_username\n    )\n    self.neo4j_password = (\n        NEO4J_PASSWORD if NEO4J_PASSWORD else neo4j_password\n    )\n\n    connection = Neo4jConnectionModel(\n        neo4j_url=self.neo4j_url,\n        neo4j_username=self.neo4j_username,\n        neo4j_password=self.neo4j_password,\n    )\n\n    self.driver = GraphDatabase.driver(\n        connection.neo4j_url,\n        auth=basic_auth(\n            connection.neo4j_username, connection.neo4j_password\n        ),\n    )\n\n    logger.debug(\n        f\"Connected to {connection.neo4j_url} database successfully\"\n    )\n</code></pre>"},{"location":"db_neo4j/#erad.db.neo4j_.Neo4J.close_driver","title":"<code>close_driver()</code>","text":"<p>Method to close the driver.</p> Source code in <code>erad\\db\\neo4j_.py</code> <pre><code>def close_driver(self):\n\"\"\"Method to close the driver.\"\"\"\n    self.driver.close()\n</code></pre>"},{"location":"db_neo4j/#erad.db.neo4j_.Neo4J.rename_labels","title":"<code>rename_labels(label)</code>  <code>staticmethod</code>","text":"<p>Method to replace the invalid character.</p> Source code in <code>erad\\db\\neo4j_.py</code> <pre><code>@staticmethod\ndef rename_labels(label):\n\"\"\"Method to replace the invalid character.\"\"\"\n    invalid_chars = [\"-\", \":\", \"(\", \")\", \".\"]\n    for invalid_char in invalid_chars:\n        if invalid_char in label:\n            label = label.replace(invalid_char, \"__\")\n    return label\n</code></pre>"},{"location":"disaster_modeling/","title":"Disaster modeling","text":"<p>Currently you can model earthquake, flooding and fire in ERAD. Fire model is still under development and documentation is not included for fire for now.</p>"},{"location":"disaster_modeling/#simulating-earthquake","title":"Simulating earthquake","text":"<p>The snippets below shows an example for simulating earthquake in ERAD. </p> <p><pre><code>import datetime \n\nfrom erad.db import neo4j_\nfrom erad.db import inject_earthquake\nfrom erad.db import disaster_input_model\n\n# Make sure to update the url, username and password\nneo4j_instance = neo4j_.Neo4J(\n                neo4j_url='bolt://localhost:7687',\n                neo4j_username='neo4j', \n                neo4j_password='neo4j')\n\n\nearthquake = disaster_input_model.PointEarthquake(\n            longitude=-121.72125955960196, \n            latitude=37.92770173811863,\n            timestamp=datetime.datetime(2022, 1, 1, 0, 0,0),\n            magnitude=6.0,\n            depth=15.0\n        )\n\ninject_earthquake.inject_point_earthquake(\n        earthquake, neo4j_instance.driver,\n        critical_infras=[\"Grocery\", \"Hospital\",\"Convenience\", \"Shelter\", \"Banking\"])\n</code></pre> We first created a neo4j database instance by passing neo4j host url, username and password. Secondly we defined input model for point earthquake by passing relevant input parameters. Lastly <code>inject_point_earthquake</code> takes the input, database driver and list of critical infrastructure which need to be updated following the simulation.</p>"},{"location":"exceptions/","title":"exceptions","text":"<p>Module for managing exceptions raised by ERAD package.</p>"},{"location":"exceptions/#erad.exceptions.DatabaseMissingInfo","title":"<code>DatabaseMissingInfo</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exception raised because information required to connect to database is missing.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class DatabaseMissingInfo(ERADBaseException):\n\"\"\"Exception raised because information required to connect to database is missing.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.DittoException","title":"<code>DittoException</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because application ran into an issus using Ditto.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class DittoException(ERADBaseException):\n\"\"\"Exceptions raised because application ran into an issus using Ditto.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.ERADBaseException","title":"<code>ERADBaseException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>All exception should derive from this.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class ERADBaseException(Exception):\n\"\"\"All exception should derive from this.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.EmptyEnvironmentVariable","title":"<code>EmptyEnvironmentVariable</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exception raised because environment variable required is empty.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class EmptyEnvironmentVariable(ERADBaseException):\n\"\"\"Exception raised because environment variable required is empty.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.EmptyScenarioPolygon","title":"<code>EmptyScenarioPolygon</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because no polygons are found.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class EmptyScenarioPolygon(ERADBaseException):\n\"\"\"Exceptions raised because no polygons are found.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.FeatureNotImplementedError","title":"<code>FeatureNotImplementedError</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exception raised because specific feature requested has not been implemented.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class FeatureNotImplementedError(ERADBaseException):\n\"\"\"Exception raised because specific feature requested has not been implemented.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.InvalidFileTypePassed","title":"<code>InvalidFileTypePassed</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because invalid file type is passed.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class InvalidFileTypePassed(ERADBaseException):\n\"\"\"Exceptions raised because invalid file type is passed.\"\"\"\n\n    def __init__(self, path, valid_type):\n\n        self.message = f\"Invalid file type of {Path(path).suffix} is passed! Please pass valid file type of {valid_type}\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"exceptions/#erad.exceptions.MultiStatePlaneError","title":"<code>MultiStatePlaneError</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because the corrdinates are in more than one state plane coordinates.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class MultiStatePlaneError(ERADBaseException):\n\"\"\"Exceptions raised because the corrdinates are in more than one state plane\n    coordinates.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.NotAFileError","title":"<code>NotAFileError</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exception raised because file is expected but folder path is provided.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class NotAFileError(ERADBaseException):\n\"\"\"Exception raised because file is expected but folder path is provided.\"\"\"\n\n    def __init__(self, path):\n        self.message = f\"Expected file path {path} is not a file!\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"exceptions/#erad.exceptions.OpenDSSCommandError","title":"<code>OpenDSSCommandError</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because opendss command execution ran into an error.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class OpenDSSCommandError(ERADBaseException):\n\"\"\"Exceptions raised because opendss command execution ran into an error.\"\"\"\n</code></pre>"},{"location":"exceptions/#erad.exceptions.PathDoesNotExist","title":"<code>PathDoesNotExist</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exception raised bacause expected file/folder path does not exist.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class PathDoesNotExist(ERADBaseException):\n\"\"\"Exception raised bacause expected file/folder path does not exist.\"\"\"\n\n    def __init__(self, path):\n        self.message = (\n            f\"Expected path {path} does not exist. please check you file path!\"\n        )\n        super().__init__(self.message)\n</code></pre>"},{"location":"exceptions/#erad.exceptions.SMARTDSInvalidInput","title":"<code>SMARTDSInvalidInput</code>","text":"<p>         Bases: <code>ERADBaseException</code></p> <p>Exceptions raised because invalid input is provided for SMART DS data download.</p> Source code in <code>erad\\exceptions.py</code> <pre><code>class SMARTDSInvalidInput(ERADBaseException):\n\"\"\"Exceptions raised because invalid input is provided for SMART DS data download.\"\"\"\n</code></pre>"},{"location":"how-to-install/","title":"Installation Instruction","text":""},{"location":"how-to-install/#package-installation","title":"Package Installation","text":"<p>We recommend using Anaconda or Miniconda to create the environment and instal ERAD.  Use the commands below to create environment and install the ERAD python package.</p>  Windows 10 +  Mac OS &amp; linux <pre><code>conda create -n erad python==3.8\nconda activate erad\nconda install shapely\npip install NREL-erad==1.0.0\n</code></pre> <pre><code>conda create -n erad python==3.8\nconda activate erad\npip install NREL-erad==1.0.0\n</code></pre>"},{"location":"how-to-install/#neo4j-installation","title":"Neo4J Installation","text":"<p>ERAD uses neo4j database for storing underlying asset data and performing computation. First step in using ERAD is to install neo4J. You can either install neo4J Desktop (You can download latest neo4J version here. ) or use docker to start up  neo4j container. You can use following command to start up a neo4j docker container. We recommend using docker to setup neo4j. Use this documentation to install docker if you have not already installed it in your machine.</p> <pre><code>docker run -d \\\n-p 7474:7474 -p 7687:7687 \\\n--env NEO4J_dbms_memory_transaction_total_max=10g \\\n--env NEO4J_server_memory_heap_max__size=10g \\\n-v /data:/data \\\n-v /plugins:/plugins \\\n-v /import:/import \\\n--name neo4j-apoc \\\n-e NEO4J_apoc_export_file_enabled=true \\\n-e NEO4J_apoc_import_file_enabled=true \\\n-e NEO4J_apoc_import_file_use__neo4j__config=true \\\n-e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\nneo4j:5.7.0-community\n</code></pre> <p>Notice volume mount for import (<code>-v /import:/import</code>). You would need to copy all the csv files (described in Loading data into database section.) before running cypher queries. You can keep the <code>plugins</code> and <code>data</code> folder empty. Make sure to update host paths for volume mount correctly before running above command. Also if your machine does not have more than 20 GB worth of RAM reduce both heap size and dbms transaction memory settings. Feel free to tune those as per your requirements.</p> <p>If you are using neo4J desktop, you will need to create new database and install <code>apoc</code> library for that database which you can do by navigating neo4J Desktop UI. Feel free to edit the setting file to upgrade memory if you are working with bigger dataset. You would need to find out depending on where you installed neo4J desktop, appropriate folder for dumping csv files as for each database neo4J creates a new folder with unique UUID. The trick I use is to run cypher query to load random csv file (Learn more about loading data using Cypher query in Loading data into database section.) in neo4j browser and looking at path in error string.</p>"},{"location":"load_data/","title":"Loading data into database","text":"<p>First step in using ERAD is preparing the dataset to load into the database. This section describes data format and the process for loading datasets into database.</p>"},{"location":"load_data/#distribution-feeder-data","title":"Distribution Feeder Data","text":"<p>One of the key dataset for simulation is distribution feeder data. You need to prepare csv files for distribution assets which will be used to bulk load the data into Neo4J database using Cypher Query. Here is a list of csv files you need to prepare for distribution system. You can also use the sample datasets available in the repository.</p> File Name Description Columns <code>buses.csv</code> CSV file containing information about distribution system buses. Required. <code>name</code>, <code>longitude</code>, <code>latitude</code>, <code>kv</code> <code>loads.csv</code> CSV file containing information about power system loads. Required. <code>source</code> refer to bus name from <code>buses.csv</code> file where load is connected to and <code>critical_load_factor</code> is any value between 0 and 1 that is used to determine critical demand for this load. <code>name</code>, <code>kw</code>, <code>kvar</code>, <code>source</code>, <code>critical_load_factor</code> <code>transformers.csv</code> CSV file containing information about transformers. Required. <code>source</code> and <code>target</code> refer to bus names from <code>buses.csv</code> file where transformer's primary and secondary terminal is connected to. <code>height_m</code> is height of transformer from ground surface in meters. <code>name</code>, <code>source</code>, <code>target</code>, <code>kva</code>, <code>height_m</code> <code>line_sections.csv</code> CSV file containing information about line sections. Required. <code>source</code> and <code>target</code> refer to bus names from <code>buses.csv</code> file where line segment's to and from node is connected to. <code>height_m</code> is height of line segment from ground surface in meters and <code>geom_type</code> could be either <code>overhead</code> or <code>underground</code>. <code>name</code>, <code>source</code>, <code>target</code>, <code>geom_type</code>, <code>ampacity</code>, <code>height_m</code>, <code>num_phase</code> <code>pv_systems.csv</code> CSV file containing information about PV systems. Optional. <code>owner</code> refers to load name from <code>load.csv</code> file who owns the solar, <code>bus</code> refers to bus name from <code>buses.csv</code> file where solar is connected to and <code>capacity</code> refers to solar installation capacity in kW. <code>name</code>, <code>owner</code>, <code>bus</code>, <code>capacity</code> <code>energy_storage.csv</code> CSV file containing information about energy storages. Optional. <code>owner</code> refers to load name from <code>load.csv</code> file who owns the energy storage, <code>bus</code> refers to bus name from <code>buses.csv</code> file where energy storage is connected to, and <code>kw</code> refers to maximum kw capacity for storage <code>name</code>, <code>owner</code>, <code>bus</code>, <code>kw</code> <code>substation.csv</code> CSV file containing information about distribution substation. Required. <code>name</code> <p>If you are using OpenDSS models, ERAD includes utility functions to extract required csv files from the model.</p> <p>Following code snippet downloads one of the SMARTDS feeder dataset and extracts asset data in csv file format in <code>assets</code> folder.</p> <pre><code>from erad.utils.ditto_utils import download_smartds_data\nfrom erad.utils.opendss_utils import extract_export_opendss_model\n\ndownload_smartds_data('P3R', '.')\nextract_export_opendss_model('P3R__2018__SFO__oedi-data-lake_opendss_no_loadshapes/Master.dss',\n                              './assets')\n</code></pre> <p>You will see <code>buses.csv</code>, <code>capacitors.csv</code>, <code>line_sections.csv</code>, <code>load.csv</code>, <code>pv_systems.csv</code>, and <code>transformers.csv</code>. Notice that the extraction process can generate extra files not used by ERAD and may extract more information than you need and this is okay as long as csv files have all the necessary columns.</p>"},{"location":"load_data/#critical-infrastructure-data","title":"Critical Infrastructure Data","text":"<p>Along with distribution feeder data, ERAD also requires critical infrastructure data. Here is a list of csv files you need to prepare for critical infrastructures. You do not need to have all the dataset and can start with any subset of these.</p> File Name Description Columns <code>medical_centers.csv</code> CSV file containing information about medicial centers within the region of interest. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <code>shelters.csv</code> CSV file containing information about shelters. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <code>pharmacies.csv</code> CSV file containing information about pharmacies within the region of interest. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <code>groceries.csv</code> CSV file containing information about groceries within the region of interest. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <code>banking.csv</code> CSV file containing information about banks within the region of interest. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <code>convenience.csv</code> CSV file containing information about convenience stores within the region of interest. Optional. <code>gid</code>, <code>name</code>, <code>source</code>, <code>longitude</code>, <code>latitude</code>, <code>kw</code>, <code>kvar</code>, <code>backup_capacity_kw</code>, <code>backup</code> <p><code>gid</code> refers to any unique global identifier, <code>source</code> is bus name from <code>buses.csv</code> file where it is connected to in the, distribution grid, <code>name</code> is a friendly name for infrastructure, <code>kw</code> is the active peak load for the infrastructure, <code>kvar</code>is the reactive peak load for the infrastructure, <code>backup_capacity_kw</code> is the backup generating source capacity for the infrastructure in kw, and <code>backup</code> is binary 0 or 1</p> <p>One of the dataset you can use to get critical infrastructure data is HIFLD dataset.</p> Infrastrcuture Name Data sources Hospitals HIFLD Open (Hospitals for all US) Shelters HIFLD Open (National Shelter System Facilities) Pharmacies HIFLD Open (Pharmacies for all US) Banking HIFLD Open (FDIC Insured Banks)"},{"location":"load_data/#bulk-loading-data-from-csv-files-into-neo4j-database","title":"Bulk loading data from csv files into Neo4J database","text":"<p>Once you have prepared all the csv files, you will need to copy these into <code>/import</code> folder if you are using Neo4J docker container. If you are using Neo4J desktop, you will need to find <code>import</code> folder for specific database you have created. Please take a look at the Neo4j doc for more details on how to bulk load csv files.</p> <p>Here is a sample cypher queries you can use to load the data from csv files. Make sure to comment sections or update as necessary if you need to before running it in Neo4J browser. You can typically visit <code>localhost:7474</code> for accessing Neo4J browser if you using Neo4J docker container if you are using Neo4J Desktop you can open Neo4J browser from the UI. It should look something like this.</p> <p></p> <pre><code>LOAD CSV WITH HEADERS FROM 'file:///buses.csv' AS bus_row \nWITH bus_row WHERE bus_row.name IS NOT NULL MERGE (bus:Bus {name: bus_row.name, longitude: toFloat(bus_row.longitude), latitude: toFloat(bus_row.latitude)});\n\nLOAD CSV WITH HEADERS FROM 'file:///loads.csv' AS load_row \nWITH load_row WHERE load_row.name IS NOT NULL MERGE (load:Load {name: load_row.name, kw: toFloat(load_row.kw), kvar: toFloat(load_row.kvar), source: load_row.source, critical_load_factor: toFloat(load_row.critical_load_factor)})\nMERGE (load_bus:Bus {name: load_row.source})\nMERGE (load)-[:CONSUMES_POWER_FROM]-&gt;(load_bus);\n\nLOAD CSV WITH HEADERS FROM 'file:///line_sections.csv' AS line_row \nWITH line_row WHERE line_row.name IS NOT NULL MERGE (from_bus:Bus {name: line_row.source})\nMERGE (to_bus:Bus {name: line_row.target})\nMERGE (from_bus)-[:CONNECTS_TO {name: line_row.name, source: line_row.source, target: line_row.target,ampacity: toFloat(line_row.ampacity),\nheight_m: toFloat(line_row.height_m), geom_type: line_row.geom_type}]-&gt;(to_bus);\n\nLOAD CSV WITH HEADERS FROM 'file:///transformers.csv' AS xfmr_row\nWITH xfmr_row WHERE xfmr_row.name IS NOT NULL MERGE (from_bus:Bus {name: xfmr_row.source})\nMERGE (to_bus:Bus {name: xfmr_row.target})\nMERGE (from_bus)-[:CONNECTS_TO {name: xfmr_row.name, source: xfmr_row.source, target: xfmr_row.target, kva: xfmr_row.kva, height_m: toFloat(xfmr_row.height_m)}]-&gt;(to_bus);\n\n\nLOAD CSV WITH HEADERS FROM 'file:///pv_systems.csv' AS pv_row\nWITH pv_row WHERE pv_row.name IS NOT NULL MERGE (sa:Solar {capacity: toFloat(pv_row.capacity), name: pv_row.name, owner: pv_row.owner})\nMERGE (ba:Bus {name: pv_row.bus})\nMERGE (lo:Load {name: pv_row.owner})\nMERGE (sa)-[:INJECTS_ACTIVE_POWER_TO]-&gt;(ba)\nMERGE (lo)-[:OWNS]-&gt;(sa);\n\nLOAD CSV WITH HEADERS FROM 'file:///energy_storage.csv' AS es_row\nWITH es_row WHERE es_row.name IS NOT NULL MERGE (ea:EnergyStorage {kw: toFloat(es_row.kw), name: es_row.name, owner:es_row.owner})\nMERGE (ba:Bus {name: es_row.bus})\nMERGE (lo:Load {name: es_row.owner})\nMERGE (ea)-[:INJECTS_POWER]-&gt;(ba)\nMERGE (ba)-[:CONSUMES_POWER]-&gt;(ea)\nMERGE (lo)-[:OWNS]-&gt;(ea);\n\nLOAD CSV WITH HEADERS FROM 'file:///substation.csv' AS sub_row\nWITH sub_row WHERE sub_row.name IS NOT NULL MERGE (b:Bus {name: sub_row.name}) SET b:Substation;\n\nMATCH (b:Bus)-[CONSUMES_POWER]-(c:Load)\nSET c.longitude = b.longitude\nSET c.latitude = b.latitude;\n\nMATCH (c:Load)\nWHERE c.latitude IS NULL\nDETACH DELETE c;\n\nLOAD CSV WITH HEADERS FROM 'file:///pharmacies.csv' AS p_row\nWITH p_row WHERE p_row.name IS NOT NULL MERGE (p:Pharmacy {name: (p_row.name + p_row.gid), source:p_row.source, kw:toFloat(p_row.kw), kvar:toFloat(p_row.kvar), backup_capacity_kw:toFloat(p_row.backup_capacity_kw),\nbackup: toInteger(p_row.backup), longitude: toFloat(p_row.longitude), latitude: toFloat(p_row.latitude)})\nWITH p\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_FOR_MEDICINE {distance: point.distance(\npoint({longitude: p.longitude, latitude:p.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(p);\n\nMATCH (p:Pharmacy)\nWITH p MATCH (b:Bus {name: p.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(p);\n\nLOAD CSV WITH HEADERS FROM 'file:///groceries.csv' AS g_row\nWITH g_row WHERE g_row.name IS NOT NULL MERGE (g:Grocery {name: (g_row.name + g_row.gid), source:g_row.source, kw:toFloat(g_row.kw), kvar:toFloat(g_row.kvar), backup_capacity_kw:toFloat(g_row.backup_capacity_kw),\nbackup: toInteger(g_row.backup),\nlongitude: toFloat(g_row.longitude), latitude: toFloat(g_row.latitude)})\nWITH g\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_FOR_GROCERIES {distance: point.distance(\npoint({longitude: g.longitude, latitude:g.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(g);\n\nMATCH (g:Grocery)\nWITH g MATCH (b:Bus {name: g.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(g);\n\nLOAD CSV WITH HEADERS FROM 'file:///medical_centers.csv' AS m_row\nWITH m_row WHERE m_row.name IS NOT NULL MERGE (m:Hospital {name: (m_row.name + m_row.gid),\nsource:m_row.source, kw:toFloat(m_row.kw), kvar:toFloat(m_row.kvar), backup_capacity_kw:toFloat(m_row.backup_capacity_kw),\nlongitude: toFloat(m_row.longitude), latitude: toFloat(m_row.latitude), backup: toInteger(m_row.backup)})\nWITH m\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_DURING_HEALTH_EMERGENCY {distance: point.distance(\npoint({longitude: m.longitude, latitude:m.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(m);\n\nMATCH (h:Hospital)\nWITH h MATCH (b:Bus {name: h.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(h);\n\nLOAD CSV WITH HEADERS FROM 'file:///banking.csv' AS b_row\nWITH b_row WHERE b_row.name IS NOT NULL MERGE (b:Banking {name: (b_row.name + b_row.gid),\nsource: b_row.source,kw:toFloat(b_row.kw), kvar:toFloat(b_row.kvar), backup_capacity_kw:toFloat(b_row.backup_capacity_kw),\nbackup: toInteger(b_row.backup),\nlongitude: toFloat(b_row.longitude), latitude: toFloat(b_row.latitude)})\nWITH b\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_TO_WITHDRAW_OR_DEPOSIT_CURRENCY {distance: point.distance(\npoint({longitude: b.longitude, latitude:b.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(b);\n\nMATCH (b1:Banking)\nWITH b1\nMATCH (b:Bus {name: b1.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(b1);\n\nLOAD CSV WITH HEADERS FROM 'file:///convenience.csv' AS c_row\nWITH c_row WHERE c_row.name IS NOT NULL MERGE (c:Convenience {name: (c_row.name + c_row.gid), source:c_row.source, kw:toFloat(c_row.kw), kvar:toFloat(c_row.kvar), backup_capacity_kw:toFloat(c_row.backup_capacity_kw),\nbackup: toInteger(c_row.backup),\nlongitude: toFloat(c_row.longitude), latitude: toFloat(c_row.latitude)})\nWITH c\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_FOR_SERVICE {distance: point.distance(\npoint({longitude: c.longitude, latitude:c.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(c);\n\nMATCH (c:Convenience)\nWITH c\nMATCH (b:Bus {name: c.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(c);\n\nLOAD CSV WITH HEADERS FROM 'file:///shelters.csv' AS s_row\nWITH s_row WHERE s_row.use_type IS NOT NULL MERGE (s:Shelter {name: (s_row.use_type + s_row.gid), backup: toInteger(s_row.backup), source: s_row.source, kw:toFloat(s_row.kw), kvar:toFloat(s_row.kvar), backup_capacity_kw:toFloat(s_row.backup_capacity_kw),\nlongitude: toFloat(s_row.longitude), latitude: toFloat(s_row.latitude)})\nWITH s\nMATCH (lo:Load)\nMERGE (lo)-[:VISITS_FOR_SERVICE {distance: point.distance(\npoint({longitude: s.longitude, latitude:s.latitude}),\npoint({longitude: lo.longitude, latitude:lo.latitude})\n)}]-&gt;(s);\n\nMATCH (s:Shelter)\nWITH s\nMATCH (b:Bus {name: s.source})\nMERGE (b)&lt;-[:GETS_POWER_FROM]-(s);\n\nMATCH (b1:Bus)-[r:CONNECTS_TO]-(b2:Bus)\nSET r.longitude = (b1.longitude + b2.longitude)/2\nSET r.latitude = (b1.latitude + b2.latitude)/2;\n\nMATCH p=(sourceNode:Bus)-[r:CONNECTS_TO]-(targetNode:Bus)\nWHERE r.ampacity IS NOT NULL\nWITH r,sourceNode, CASE r.num_phase\nWHEN 3 THEN 1.732\nELSE 1\nEND AS multiplier\nSET r.kva = multiplier*r.ampacity*sourceNode.kv;\n\nMATCH p=(sourceNode:Bus)-[r:CONSUMES_POWER_FROM]-(targetNode:Load)\nSET targetNode.kw = toFloat(targetNode.kw)\nSET targetNode.kvar = toFloat(targetNode.kvar)\nSET r.kva = sqrt(targetNode.kw*targetNode.kw+targetNode.kvar*targetNode.kvar);\n\nMATCH p=(sourceNode:Bus)-[r:INJECTS_ACTIVE_POWER_TO]-(targetNode:Solar)\nSET targetNode.capacity = toFloat(targetNode.capacity)\nSET r.kva = targetNode.capacity;\n\n\nMATCH p=(sourceNode:Bus)-[r:INJECTS_POWER]-(targetNode:EnergyStorage)\nSET targetNode.kw = toFloat(targetNode.kw)\nSET r.kva = targetNode.kw;\n\nMATCH (sourceNode:Bus)-[r:CONNECTS_TO]-(targetNode:Bus)\nSET r.kva = toFloat(r.kva);\n\nMATCH p=()-[r:GETS_POWER_FROM]-&gt;()\nWHERE r.kva is null\nSET r.kva = 300;\n\nMATCH p=()-[r:CONNECTS_TO]-&gt;() WHERE r.kva IS null\nSET r.kva = 300;\n</code></pre>"},{"location":"notebook_examples/","title":"Notebooks Examples","text":"<p>List of notebooks available in the repository.</p> <ul> <li>Demo flooding simulation: This example shows how to simulate flooding in ERAD and use calculate survival probability function to update survival probability for distribution system assets. Note this example does not use Neo4J.</li> </ul>"},{"location":"utility_functions/","title":"Utility Scripts","text":""},{"location":"utility_functions/#downloading-the-smartds-opendss-feeder-model","title":"Downloading the SMARTDS opendss feeder model","text":"<p>Use the following code snippet to download the OpenDSS model for the region you are interested in. Just provide the region name and where you would like the downloaded files to be stored.</p> <pre><code>from utils.ditto_utils import download_smartds_data\ndownload_smartds_data('P4R',  r'./smartds_opendss_models')\n</code></pre>"},{"location":"utility_functions/#parsing-generic-opendss-model-to-retrieve-metadata","title":"Parsing generic opendss model to retrieve metadata","text":"<p>The command below takes master opendss file and folder where you want to store the extracted data. It will generate csvs for distribution assets.</p> <pre><code>from utils.opendss_utils import extract_opendss_model\nextract_opendss_model(\n        r'smartds_opendss_models\\P4R__2018__SFO__oedi-data-lake_opendss_no_loadshapes\\Master.dss',\n        r'./graph_csvs'\n    )\n</code></pre>"},{"location":"utility_functions/#preparing-hifld-data-for-the-region-of-interest","title":"Preparing HIFLD data for the region of interest","text":"<p>First download the HIFLD CSV data set and use the following command to get the subset of the csv for the region you are interedted in. First create a bounding box with 5km buffer. Then use the bounding box corners to filter the csv data set.</p> <pre><code>from utils.opendss_utils import get_bounding_box\nfrom utils.hifld import get_subset_of_hifld_data\nbounds = get_bounding_box(\n    r'smartds_opendss_models\\P4R__2018__SFO__oedi-data-lake_opendss_no_loadshapes\\Master.dss', \n    5000)\n\nget_subset_of_hifld_data(\n    r'./Emergency_Medical_Service_(EMS)_Stations.csv',\n    bounds,\n    r'./graph_csvs',\n    logitude_column_name='LONGITUDE',\n    latitude_column_name='LATITUDE',\n    columns_to_keep=['LONGITUDE', 'LATITUDE', \n    'STATE', 'ZIP', 'CITY', 'COUNTY'],\n    name_of_csv_file='medical_centers.csv')\n</code></pre>"},{"location":"utils_ditto_utils/","title":"utils.ditto_utils","text":"<p>Utility functions for dealing with SMART DS dataset. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from erad import ditto_utils\n&gt;&gt;&gt; ditto_utils.download_smartds_data('P4R',  '.')\n</code></pre>"},{"location":"utils_ditto_utils/#erad.utils.ditto_utils.create_networkx_from_ditto","title":"<code>create_networkx_from_ditto(output_path, file_name, **kwargs)</code>","text":"<p>Creates networkx graph from OpenDSS model using Ditto.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to store the networkx data in json file format</p> required <code>file_name</code> <code>str</code> <p>JSON file name used to export the network</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments accepted by Ditto</p> <code>{}</code> Source code in <code>erad\\utils\\ditto_utils.py</code> <pre><code>def create_networkx_from_ditto(\n    output_path: str, file_name: str, **kwargs\n) -&gt; None:\n\"\"\"Creates networkx graph from OpenDSS model using Ditto.\n\n    Args:\n        output_path (str): Path to store the networkx\n            data in json file format\n        file_name (str): JSON file name used to export\n            the network\n        kwargs (dict): Keyword arguments accepted\n            by Ditto\n    \"\"\"\n    try:\n        output_path = Path(output_path)\n        return _create_networkx_from_ditto(output_path, file_name, **kwargs)\n    finally:\n        for file_path in output_path.iterdir():\n            if file_path.suffix == \".adjlist\":\n                file_path.unlink(missing_ok=True)\n</code></pre>"},{"location":"utils_ditto_utils/#erad.utils.ditto_utils.create_networkx_from_json","title":"<code>create_networkx_from_json(json_file_path)</code>","text":"<p>Returns networkx graph from JSON file.</p> Source code in <code>erad\\utils\\ditto_utils.py</code> <pre><code>def create_networkx_from_json(json_file_path: str):\n\"\"\"Returns networkx graph from JSON file.\"\"\"\n    content = read_file(json_file_path)\n    return json_graph.adjacency_graph(content)\n</code></pre>"},{"location":"utils_ditto_utils/#erad.utils.ditto_utils.download_aws_dir","title":"<code>download_aws_dir(bucket, path, target, unsigned=True, **kwargs)</code>","text":"<p>Utility function download data from AWS S3 directory.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>path</code> <code>str</code> <p>S3 bucket prefix</p> required <code>target</code> <code>str</code> <p>Path for downloading the data</p> required <code>unsigned</code> <code>bool</code> <p>Indicate whether to use credential or not</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Keyword arguments accepted by <code>boto3.client</code></p> <code>{}</code> Source code in <code>erad\\utils\\ditto_utils.py</code> <pre><code>@timeit\ndef download_aws_dir(\n    bucket: str, path: str, target: str, unsigned=True, **kwargs\n) -&gt; None:\n\"\"\"Utility function download data from AWS S3 directory.\n\n    Args:\n        bucket (str): Name of the bucket.\n        path (str): S3 bucket prefix\n        target (str): Path for downloading the data\n        unsigned (bool): Indicate whether to use credential or not\n        kwargs (dict): Keyword arguments accepted by `boto3.client`\n    \"\"\"\n\n    target = Path(target)\n    if unsigned:\n        client = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n    else:\n        if kwargs:\n            client = boto3.client(\"s3\", **kwargs)\n        else:\n            client = boto3.client(\"s3\")\n\n    # Handle missing / at end of prefix\n    if not path.endswith(\"/\"):\n        path += \"/\"\n\n    paginator = client.get_paginator(\"list_objects_v2\")\n    for result in paginator.paginate(Bucket=bucket, Prefix=path):\n\n        # Download each file individually\n        for key in result[\"Contents\"]:\n\n            # Calculate relative path\n            rel_path = key[\"Key\"][len(path) :]\n\n            # Skip paths ending in /\n            if not key[\"Key\"].endswith(\"/\"):\n                local_file_path = target / rel_path\n                local_file_path.parent.mkdir(parents=True, exist_ok=True)\n                client.download_file(bucket, key[\"Key\"], str(local_file_path))\n</code></pre>"},{"location":"utils_ditto_utils/#erad.utils.ditto_utils.download_smartds_data","title":"<code>download_smartds_data(smartds_region, output_path='./smart_ds_downloads', year=2018, area='SFO', s3_bucket_name='oedi-data-lake', folder_name='opendss_no_loadshapes', cache_folder='cache')</code>","text":"<p>Utility function to download SMARTDS data from AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>smartds_region</code> <code>str</code> <p>SMARTDS region name</p> required <code>output_path</code> <code>str</code> <p>Path for downloaded data</p> <code>'./smart_ds_downloads'</code> <code>year</code> <code>int</code> <p>Valid year input for downloading the data</p> <code>2018</code> <code>area</code> <code>str</code> <p>Valid SMARTDS area</p> <code>'SFO'</code> <code>s3_bucket_name</code> <code>str</code> <p>S3 bucket name storing the SMARTDS data</p> <code>'oedi-data-lake'</code> <code>folder_name</code> <code>str</code> <p>S3 bucket folder to download</p> <code>'opendss_no_loadshapes'</code> <code>cache_folder</code> <code>str</code> <p>Folder path for caching the results</p> <code>'cache'</code> <p>Raises:</p> Type Description <code>SMARTDSInvalidInput</code> <p>Raises this error if year and/or area provided is not valid.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Folder path containing downloaded data.</p> Source code in <code>erad\\utils\\ditto_utils.py</code> <pre><code>@timeit\ndef download_smartds_data(\n    smartds_region: str,\n    output_path: str = \"./smart_ds_downloads\",\n    year: int = 2018,\n    area: str = \"SFO\",\n    s3_bucket_name: str = \"oedi-data-lake\",\n    folder_name: str = \"opendss_no_loadshapes\",\n    cache_folder: str = \"cache\",\n) -&gt; str:\n\"\"\"Utility function to download SMARTDS data from AWS S3 bucket.\n\n    Args:\n        smartds_region (str): SMARTDS region name\n        output_path (str): Path for downloaded data\n        year (int): Valid year input for downloading the data\n        area (str): Valid SMARTDS area\n        s3_bucket_name (str): S3 bucket name storing the SMARTDS data\n        folder_name (str): S3 bucket folder to download\n        cache_folder (str): Folder path for caching the results\n\n    Raises:\n        SMARTDSInvalidInput: Raises this error if year and/or area\n            provided is not valid.\n\n    Returns:\n        str: Folder path containing downloaded data.\n    \"\"\"\n    if year not in SMARTDS_VALID_YEARS or area not in SMARTDS_VALID_AREAS:\n        raise SMARTDSInvalidInput(\n            f\"Not valid input! year= {year} area={area}, \\\n            valid_years={SMARTDS_VALID_YEARS}, valid_areas={SMARTDS_VALID_AREAS}\"\n        )\n\n    output_path = Path(output_path)\n    cache_folder = Path(cache_folder)\n\n    output_path.mkdir(exist_ok=True)\n    cache_folder.mkdir(exist_ok=True)\n\n    cache_key = (\n        f\"{smartds_region}__{year}__{area}__{s3_bucket_name}_{folder_name}\"\n    )\n    cache_data_folder = cache_folder / cache_key\n    output_folder = output_path / cache_key\n\n    if cache_data_folder.exists():\n        logger.info(f\"Cache hit for {cache_data_folder}\")\n        shutil.copytree(cache_data_folder, output_folder, dirs_exist_ok=True)\n\n    else:\n        logger.info(\n            f\"Cache missed reaching to AWS for downloading the data ...\"\n        )\n        output_folder.mkdir(exist_ok=True)\n        prefix = f\"SMART-DS/v1.0/{year}/{area}/{smartds_region}/scenarios/base_timeseries/{folder_name}/\"\n        download_aws_dir(s3_bucket_name, prefix, output_folder)\n        shutil.copytree(output_folder, cache_data_folder, dirs_exist_ok=False)\n\n    logger.info(f\"Check the folder {output_folder} for downloaded data\")\n    return output_folder\n</code></pre>"},{"location":"utils_hifld_utils/","title":"utils.hifld_utils","text":"<p>Module for parsing Homeland infrastructure foundation level-data.</p> <p>Idea is to take the bounding box and find the subset of  infrastructure in that region.</p>"},{"location":"utils_hifld_utils/#erad.utils.hifld_utils.get_relationship_between_hifld_infrastructures","title":"<code>get_relationship_between_hifld_infrastructures(hifld_data_csv, unique_id_column, load_csv, bus_csv, output_csv_path, distance_threshold=2000.0)</code>","text":"<p>Creates a relationship between consumers and HIFLD infrastructures.</p> <p>Parameters:</p> Name Type Description Default <code>hifld_data_csv</code> <code>str</code> <p>Path to filtered HIFLD data csv file</p> required <code>unique_id_column</code> <code>List</code> <p>Column name used as identifier for critical infrastructures</p> required <code>load_csv</code> <code>str</code> <p>Path to load csv file</p> required <code>bus_csv</code> <code>str</code> <p>Path to bus csv file</p> required <code>output_csv_path</code> <code>str</code> <p>output csv path for storing relationship csv</p> required <code>distance_threshold</code> <code>float</code> <p>Distance threshold used for mapping customer to critical infrastructure</p> <code>2000.0</code> Source code in <code>erad\\utils\\hifld_utils.py</code> <pre><code>def get_relationship_between_hifld_infrastructures(\n    hifld_data_csv: str,\n    unique_id_column: str,\n    load_csv: str,\n    bus_csv: str,\n    output_csv_path: str,\n    distance_threshold: float = 2000.0,\n):\n\"\"\"Creates a relationship between consumers and HIFLD infrastructures.\n\n    Args:\n        hifld_data_csv (str): Path to filtered HIFLD data csv file\n        unique_id_column (List): Column name used as identifier\n            for critical infrastructures\n        load_csv (str): Path to load csv file\n        bus_csv (str): Path to bus csv file\n        output_csv_path (str): output csv path for storing relationship csv\n        distance_threshold (float): Distance threshold used for mapping\n            customer to critical infrastructure\n    \"\"\"\n    hifld_data_csv = Path(hifld_data_csv)\n    bus_csv = Path(bus_csv)\n    load_csv = Path(load_csv)\n    output_csv_path = Path(output_csv_path)\n\n    path_validation(\n        hifld_data_csv, check_for_file=True, check_for_file_type=\".csv\"\n    )\n    path_validation(bus_csv, check_for_file=True, check_for_file_type=\".csv\")\n    path_validation(load_csv, check_for_file=True, check_for_file_type=\".csv\")\n    path_validation(output_csv_path.parents[0])\n\n    hifld_data_df = pd.read_csv(hifld_data_csv)\n    load_df = pd.read_csv(load_csv)\n    bus_df = pd.read_csv(bus_csv)\n\n    merged_data = pd.merge(\n        load_df, bus_df, how=\"left\", left_on=\"source\", right_on=\"name\"\n    ).to_dict(orient=\"records\")\n\n    # Container for storing shelter relationships\n    _relationship = []\n    for _record in hifld_data_df.to_dict(orient=\"records\"):\n        _lon, _lat = _record[\"LONGITUDE\"], _record[\"LATITUDE\"]\n\n        # convert into state plane coordinates\n        _lon_translated, _lat_translated = stateplane.from_lonlat(_lon, _lat)\n\n        # Loop through all the loads\n        for load_record in merged_data:\n\n            load_lon, load_lat = (\n                load_record[\"longitude\"],\n                load_record[\"latitude\"],\n            )\n\n            # convert into state plane coordinates\n            load_lon_translated, load_lat_translated = stateplane.from_lonlat(\n                load_lon, load_lat\n            )\n\n            # computes distance\n            distance = math.sqrt(\n                (_lat_translated - load_lat_translated) ** 2\n                + (_lon_translated - load_lon_translated) ** 2\n            )\n\n            if distance &lt; distance_threshold:\n                _relationship.append(\n                    {\n                        unique_id_column: _record[unique_id_column],\n                        \"load_name\": load_record[\"name_x\"],\n                        \"distance\": distance,\n                    }\n                )\n\n    df = pd.DataFrame(_relationship)\n    df.to_csv(output_csv_path)\n</code></pre>"},{"location":"utils_hifld_utils/#erad.utils.hifld_utils.get_subset_of_hifld_data","title":"<code>get_subset_of_hifld_data(csv_file, bounds, output_folder, logitude_column_name='X', latitude_column_name='Y', columns_to_keep=['X', 'Y'], name_of_csv_file=None)</code>","text":"<p>Extracts a subset of HIFLD data set.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>Path to HIFLD data csv file</p> required <code>bounds</code> <code>List</code> <p>Bounding box coordinates</p> required <code>output_folder</code> <code>str</code> <p>Path to output folder</p> required <code>logitude_column_name</code> <code>str</code> <p>Expects column with name 'X'</p> <code>'X'</code> <code>latitude_column_name</code> <code>str</code> <p>Expects column with name 'Y'</p> <code>'Y'</code> <code>columns_to_keep</code> <code>List</code> <p>List of column names to keep by default keeps all of them</p> <code>['X', 'Y']</code> <p>name_of_csv_file (Union[str, None]): Name of csv file to export         filtered set</p> Source code in <code>erad\\utils\\hifld_utils.py</code> <pre><code>def get_subset_of_hifld_data(\n    csv_file: str,\n    bounds: List,\n    output_folder: str,\n    logitude_column_name: str = \"X\",\n    latitude_column_name: str = \"Y\",\n    columns_to_keep: List[str] = [\"X\", \"Y\"],\n    name_of_csv_file: Union[str, None] = None,\n) -&gt; None:\n\"\"\"Extracts a subset of HIFLD data set.\n\n    Args:\n        csv_file (str): Path to HIFLD data csv file\n        bounds (List): Bounding box coordinates\n        output_folder (str): Path to output folder\n        logitude_column_name (str): Expects column with name 'X'\n        latitude_column_name (str): Expects column with name 'Y'\n        columns_to_keep (List): List of column names to keep\n            by default keeps all of them\n       name_of_csv_file (Union[str, None]): Name of csv file to export\n            filtered set\n    \"\"\"\n\n    # Unpacking the bounds data\n    longitude_min, latitude_min, longitude_max, latitude_max = bounds\n\n    # Do a path validation\n    csv_file = Path(csv_file)\n    output_folder = Path(output_folder)\n    path_validation(csv_file, check_for_file=True, check_for_file_type=\".csv\")\n    path_validation(output_folder)\n\n    # Reading the hifld csv data\n    df = pd.read_csv(csv_file)\n\n    # filtering for bounds\n    df_filtered = df[\n        (df[logitude_column_name] &gt;= longitude_min)\n        &amp; (df[logitude_column_name] &lt;= longitude_max)\n        &amp; (df[latitude_column_name] &gt;= latitude_min)\n        &amp; (df[latitude_column_name] &lt;= latitude_max)\n    ]\n\n    # Keep only the limited columns\n    df_subset = df_filtered[columns_to_keep]\n\n    # export the subset\n    file_name = name_of_csv_file if name_of_csv_file else csv_file.name\n    df_subset.to_csv(output_folder / file_name)\n</code></pre>"},{"location":"utils_opendss_utils/","title":"utils.opendss_utils","text":"<p>Module for extracting assets from OpenDSS model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from erad import utils\n&gt;&gt;&gt; extract_opendss_model(\n        &lt;path_to_opendss_master_file&gt;,\n        &lt;output_folder&gt;\n    )\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.execute_dss_command","title":"<code>execute_dss_command(dss_instance, dss_command)</code>","text":"<p>Pass the valid dss command to be executed.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <code>dss_command</code> <code>str</code> <p>DSS command sring to be executed</p> required <p>Raises:</p> Type Description <code>OpenDSSCommandError</code> <p>Raises this because opendss command execution ran into error</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def execute_dss_command(dss_instance: dss, dss_command: str) -&gt; None:\n\"\"\"Pass the valid dss command to be executed.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n        dss_command (str): DSS command sring to be executed\n\n    Raises:\n        OpenDSSCommandError: Raises this because opendss command execution\n            ran into error\n\n    \"\"\"\n    error = dss_instance.run_command(dss_command)\n    if error:\n        logger.error(f\"Error executing command {dss_command} &gt;&gt; {error}\")\n        raise OpenDSSCommandError(\n            f\"Error executing command {dss_command} &gt;&gt; {error}\"\n        )\n    logger.info(f\"Sucessfully executed the command, {dss_command}\")\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.extract_export_opendss_model","title":"<code>extract_export_opendss_model(master_file, output_folder_path)</code>","text":"<p>Extract the opendss models and exports into csv file format.</p> <p>Parameters:</p> Name Type Description Default <code>master_file</code> <code>str</code> <p>Path to opendss master file</p> required <code>output_folder_path</code> <code>str</code> <p>Folder path for exporting the models to.</p> required Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def extract_export_opendss_model(\n    master_file: str, output_folder_path: str\n) -&gt; None:\n\"\"\"Extract the opendss models and exports into csv file format.\n\n    Args:\n        master_file (str): Path to opendss master file\n        output_folder_path (str): Folder path for exporting the models to.\n    \"\"\"\n\n    # Do a basic check on the path\n    master_file = Path(master_file)\n    path_validation(master_file)\n    logger.debug(f\"Attempting to read case file &gt;&gt; {master_file}\")\n\n    # Clear memory and compile dss file\n    dss.run_command(\"Clear\")\n    dss.Basic.ClearAll()\n    execute_dss_command(dss, f\"Redirect {master_file}\")\n\n    # Initial container\n    transformers = get_transformers(dss)\n    line_sections = get_line_sections(dss)\n    buses = get_buses(dss)\n    capacitors = get_capacitors(dss)\n    pv_systems = get_pvsystems(dss)\n    loads = get_loads(dss)\n\n    output_folder_path = Path(output_folder_path)\n    output_folder_path.mkdir(exist_ok=True)\n    transformers_df = pd.DataFrame(transformers)\n    transformers_df.to_csv(output_folder_path / \"transformers.csv\")\n\n    line_sections_df = pd.DataFrame(line_sections)\n    line_sections_df.to_csv(output_folder_path / \"line_sections.csv\")\n\n    buses_df = pd.DataFrame(buses)\n    buses_df.to_csv(output_folder_path / \"buses.csv\")\n\n    capacitors_df = pd.DataFrame(capacitors)\n    capacitors_df.to_csv(output_folder_path / \"capacitors.csv\")\n\n    pv_systems_df = pd.DataFrame(pv_systems)\n    pv_systems_df.to_csv(output_folder_path / \"pv_systems.csv\")\n\n    loads_df = pd.DataFrame(loads)\n    loads_df.to_csv(output_folder_path / \"loads.csv\")\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_bounding_box","title":"<code>get_bounding_box(master_file, buffer=1000)</code>","text":"<p>Creates a bounding box coordinate for covering region of opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>master_file</code> <code>str</code> <p>Path to master dss file</p> required <code>buffer</code> <code>float</code> <p>Buffer distance around distribution model in meter</p> <code>1000</code> <p>Raises:</p> Type Description <code>MultiStatePlaneError</code> <p>Raises this if opendss models lies in multiple state plane coordinates</p> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of bounding box coordinates (lower_left, upper_right)</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_bounding_box(master_file: str, buffer: float = 1000) -&gt; List:\n\"\"\"Creates a bounding box coordinate for covering region of opendss model.\n\n    Args:\n        master_file (str): Path to master dss file\n        buffer (float): Buffer distance around distribution model in meter\n\n    Raises:\n        MultiStatePlaneError: Raises this if opendss models lies in multiple\n            state plane coordinates\n\n    Returns:\n        List: List of bounding box coordinates (lower_left, upper_right)\n    \"\"\"\n\n    # Get bounding box for opendss network\n    # Do a basic check on the path\n    master_file = Path(master_file)\n    path_validation(master_file)\n    logger.debug(f\"Attempting to read case file &gt;&gt; {master_file}\")\n\n    # Clear memory and compile dss file\n    dss.run_command(\"Clear\")\n    dss.Basic.ClearAll()\n    execute_dss_command(dss, f\"Redirect {master_file}\")\n\n    # Get all the points\n    points = []\n    for bus in dss.Circuit.AllBusNames():\n        dss.Circuit.SetActiveBus(bus)\n        points.append([dss.Bus.X(), dss.Bus.Y()])\n\n    # Create a multipoint to get bounds\n    multi_points = MultiPoint(points)\n    bounds = multi_points.bounds\n\n    # Get EPSG value for converting into coordinate reference system\n    if stateplane.identify(bounds[0], bounds[1]) != stateplane.identify(\n        bounds[2], bounds[3]\n    ):\n        raise MultiStatePlaneError(\n            f\"The regions uses multiple stateplane coordinate system\"\n        )\n\n    epsg_value = stateplane.identify(bounds[0], bounds[1])\n\n    # Let's project all the WGS84 coordinates into\n    # transformed coordinates this will make sure distance is in meter\n    transformed_points = [\n        stateplane.from_lonlat(*point, epsg_value) for point in points\n    ]\n\n    # Create a multipoint from the transformed coordinates\n    transformed_multipoint = MultiPoint(transformed_points).buffer(buffer)\n\n    # Get the bounds and convert back to wsg84 format\n    transformed_bounds = transformed_multipoint.bounds\n    bounds_wsg84 = stateplane.to_lonlat(\n        transformed_bounds[0], transformed_bounds[1], epsg_value\n    ) + stateplane.to_lonlat(\n        transformed_bounds[2], transformed_bounds[3], epsg_value\n    )\n\n    return bounds_wsg84\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_buses","title":"<code>get_buses(dss_instance)</code>","text":"<p>Function to return list of all buses in opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of bus metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_buses(dss_instance: dss) -&gt; List:\n\"\"\"Function to return list of all buses in opendss model.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of bus metadata object\n    \"\"\"\n    buses_container = []\n    for bus in dss_instance.Circuit.AllBusNames():\n        dss_instance.Circuit.SetActiveBus(bus)\n        buses_container.append(\n            {\n                \"name\": bus,\n                \"type\": \"Bus\",\n                \"kv\": dss_instance.Bus.kVBase(),\n                \"longitude\": dss_instance.Bus.X(),\n                \"latitude\": dss_instance.Bus.Y(),\n            }\n        )\n    return buses_container\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_capacitors","title":"<code>get_capacitors(dss_instance)</code>","text":"<p>Function to return list of all capacitors in opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of capacitor metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_capacitors(dss_instance: dss) -&gt; List:\n\"\"\"Function to return list of all capacitors in opendss model.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of capacitor metadata object\n    \"\"\"\n\n    capacitors_container = []\n    flag = dss_instance.Capacitors.First()\n    while flag &gt; 0:\n        capacitor_name = dss_instance.CktElement.Name().lower()\n        buses = dss_instance.CktElement.BusNames()\n        bus1 = buses[0].split(\".\")[0]\n\n        capacitors_container.append(\n            {\n                \"name\": capacitor_name,\n                \"type\": \"Capacitor\",\n                \"source\": bus1,\n                \"kv\": dss_instance.Capacitors.kV(),\n                \"kvar\": dss_instance.Capacitors.kvar(),\n            }\n        )\n\n        flag = dss_instance.Capacitors.Next()\n    return capacitors_container\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_line_sections","title":"<code>get_line_sections(dss_instance)</code>","text":"<p>Function to return list of all line segments in opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of line segment metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_line_sections(dss_instance: dss) -&gt; List:\n\"\"\"Function to return list of all line segments in opendss model.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of line segment metadata object\n    \"\"\"\n    UNIT_MAPPER = {\n        0: 0,\n        1: 1.60934,\n        2: 0.3048,\n        3: 1,\n        4: 0.001,\n        5: 0.0003048,\n        6: 0.0000254,\n        7: 0.00001,\n    }\n\n    sections_container = []\n    flag = dss_instance.Lines.First()\n    while flag &gt; 0:\n        section_name = dss_instance.CktElement.Name().lower()\n        buses = dss_instance.CktElement.BusNames()\n        bus1, bus2 = buses[0].split(\".\")[0], buses[1].split(\".\")[0]\n\n        sections_container.append(\n            {\n                \"name\": section_name,\n                \"type\": \"LineSegment\",\n                \"source\": bus1,\n                \"target\": bus2,\n                \"length_km\": UNIT_MAPPER[dss_instance.Lines.Units()]\n                * dss_instance.Lines.Length(),\n                \"ampacity\": dss_instance.Lines.NormAmps(),\n                \"num_phase\": dss_instance.CktElement.NumPhases(),\n            }\n        )\n\n        flag = dss_instance.Lines.Next()\n    return sections_container\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_loads","title":"<code>get_loads(dss_instance)</code>","text":"<p>Function to return list of all loads in opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <p>List of load metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_loads(dss_instance):\n\"\"\"Function to return list of all loads in opendss model.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of load metadata object\n    \"\"\"\n\n    loads_container = []\n    flag = dss_instance.Loads.First()\n    while flag &gt; 0:\n        load_name = dss_instance.CktElement.Name().lower()\n        buses = dss_instance.CktElement.BusNames()\n        bus1 = buses[0].split(\".\")[0]\n\n        loads_container.append(\n            {\n                \"name\": load_name,\n                \"type\": \"Load\",\n                \"source\": bus1,\n                \"kw\": dss_instance.Loads.kW(),\n                \"kvar\": dss_instance.Loads.kvar(),\n            }\n        )\n\n        flag = dss_instance.Loads.Next()\n    return loads_container\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_pvsystems","title":"<code>get_pvsystems(dss_instance)</code>","text":"<p>Function to return list of all pv systems in opendss model.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of PVsystem metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_pvsystems(dss_instance: dss) -&gt; List:\n\"\"\"Function to return list of all pv systems in opendss model.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of PVsystem metadata object\n    \"\"\"\n\n    pvs_container = []\n    flag = dss_instance.PVsystems.First()\n    while flag &gt; 0:\n        pv_name = dss_instance.CktElement.Name().lower()\n        buses = dss_instance.CktElement.BusNames()\n        bus1 = buses[0].split(\".\")[0]\n\n        pvs_container.append(\n            {\n                \"name\": pv_name,\n                \"type\": \"PVSystem\",\n                \"source\": bus1,\n                \"rated_power\": dss_instance.PVSystems.Pmpp(),\n            }\n        )\n\n        flag = dss_instance.PVsystems.Next()\n    return pvs_container\n</code></pre>"},{"location":"utils_opendss_utils/#erad.utils.opendss_utils.get_transformers","title":"<code>get_transformers(dss_instance)</code>","text":"<p>Function to return list of transformers in opendss models.</p> <p>Parameters:</p> Name Type Description Default <code>dss_instance</code> <code>dss</code> <p>OpenDSS instance with models preloaded</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of transformer metadata object</p> Source code in <code>erad\\utils\\opendss_utils.py</code> <pre><code>def get_transformers(dss_instance: dss) -&gt; List:\n\"\"\"Function to return list of transformers in opendss models.\n\n    Args:\n        dss_instance (dss): OpenDSS instance with models preloaded\n\n    Returns:\n        List: List of transformer metadata object\n    \"\"\"\n\n    transformer_container = []\n    flag = dss_instance.Transformers.First()\n    while flag &gt; 0:\n        trans_name = dss_instance.CktElement.Name().lower()\n        buses = dss_instance.CktElement.BusNames()\n        bus1, bus2 = buses[0].split(\".\")[0], buses[1].split(\".\")[0]\n\n        transformer_container.append(\n            {\n                \"name\": trans_name,\n                \"type\": \"Transformer\",\n                \"source\": bus1,\n                \"target\": bus2,\n                \"kva\": dss_instance.Transformers.kVA(),\n                \"num_phase\": dss_instance.CktElement.NumPhases(),\n            }\n        )\n        flag = dss_instance.Transformers.Next()\n    return transformer_container\n</code></pre>"},{"location":"utils_util/","title":"utils.util","text":"<p>Utility functions that can be used in various parts of the code.</p>"},{"location":"utils_util/#erad.utils.util.path_validation","title":"<code>path_validation(file_path, check_for_file=False, check_for_file_type=None)</code>","text":"<p>Utility function for validating the path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to be validated</p> required <code>check_for_file</code> <code>bool</code> <p>Checks for existence of file</p> <code>False</code> <code>check_for_file_type</code> <code>Union[str, None]</code> <p>Check if file is of this type</p> <code>None</code> <p>Raises:</p> Type Description <code>PathDoesNotExist</code> <p>Raises if path does not exist</p> <code>NotAFileError</code> <p>Raises if file is not present</p> <code>InvalidFileTypePassed</code> <p>Raises if invalid file type is passed</p> Source code in <code>erad\\utils\\util.py</code> <pre><code>def path_validation(\n    file_path: str,\n    check_for_file: bool = False,\n    check_for_file_type: Union[str, None] = None,\n) -&gt; None:\n\"\"\"Utility function for validating the path.\n\n    Args:\n        file_path (str): Path to be validated\n        check_for_file (bool): Checks for existence of file\n        check_for_file_type (Union[str, None]): Check if file is of\n            this type\n\n    Raises:\n        PathDoesNotExist: Raises if path does not exist\n        NotAFileError: Raises if file is not present\n        InvalidFileTypePassed: Raises if invalid file type is passed\n    \"\"\"\n\n    file_path = Path(file_path)\n    if not file_path.exists():\n        logger.error(f\"{file_path} does not exist!\")\n        raise PathDoesNotExist(file_path)\n\n    if check_for_file and file_path.is_dir():\n        logger.error(f\"Expected file but got folder : {file_path} \")\n        raise NotAFileError(file_path)\n\n    if check_for_file_type and file_path.suffix != check_for_file_type:\n        raise InvalidFileTypePassed(file_path, check_for_file_type)\n\n    logger.debug(f\"{file_path} validated successfully!\")\n</code></pre>"},{"location":"utils_util/#erad.utils.util.read_file","title":"<code>read_file(file_path)</code>","text":"<p>Utility function to read file into a python dict.</p> <p>Supports json, yaml and geojson.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to a file to be read.</p> required <p>Raises:</p> Type Description <code>FeatureNotImplementedError</code> <p>Raises if invalid file is passed.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Python dict containing content of file.</p> Source code in <code>erad\\utils\\util.py</code> <pre><code>@timeit\ndef read_file(file_path: str) -&gt; dict:\n\"\"\"Utility function to read file into a python dict.\n\n    Supports json, yaml and geojson.\n\n    Args:\n        file_path (str): Path to a file to be read.\n\n    Raises:\n        FeatureNotImplementedError: Raises if invalid file is passed.\n\n    Returns:\n        dict: Python dict containing content of file.\n    \"\"\"\n\n    file_path = Path(file_path)\n    logger.debug(f\"Attempting to read {file_path}\")\n\n    path_validation(file_path, check_for_file=True)\n\n    # Handle JSON file read\n    if file_path.suffix == \".json\":\n        with open(file_path, \"r\") as f:\n            content = json.load(f)\n\n    # Handle YAML file read\n    elif file_path.suffix == \".yaml\":\n        with open(file_path, \"r\") as f:\n            content = yaml.safe_load(f)\n\n    # Handle geojson file read\n    elif file_path.suffix == \".geojson\":\n        with open(file_path, \"r\") as f:\n            content = geojson.load(f)\n\n    else:\n        logger.error(\n            f\"Could not read the {file_path}, this feature is not yet implemented\"\n        )\n        raise FeatureNotImplementedError(\n            f\"File of type {file_path.suffix} \\\n            is not yet implemented for reading purpose\"\n        )\n\n    logger.debug(f\"{file_path} read successfully\")\n    return content\n</code></pre>"},{"location":"utils_util/#erad.utils.util.setup_logging","title":"<code>setup_logging(filename=None)</code>","text":"<p>Creates log directory and sets up logging via logging.yaml.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to logging.yaml file</p> <code>None</code> <p>If not providex expects log file in the root of repo.</p> Source code in <code>erad\\utils\\util.py</code> <pre><code>def setup_logging(filename: Union[str, None] = None) -&gt; None:\n\"\"\"Creates log directory and sets up logging via logging.yaml.\n\n    Args:\n        filename (str): Path to logging.yaml file\n\n    If not providex expects log file in the root of repo.\n    \"\"\"\n\n    if filename is None:\n        filename = Path(__file__).parents[2] / \"logging.yaml\"\n\n    logging.config.dictConfig(read_file(filename))\n</code></pre>"},{"location":"utils_util/#erad.utils.util.timeit","title":"<code>timeit(func)</code>","text":"<p>Decorator for timing execution of a function.</p> Source code in <code>erad\\utils\\util.py</code> <pre><code>def timeit(func):\n\"\"\"Decorator for timing execution of a function.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        time_start = time.perf_counter()\n        logger.debug(f\"Timing for {func} started\")\n        ret_val = func(*args, **kwargs)\n        time_elapsed = time.perf_counter() - time_start\n        # memory_mb =resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0\n        logger.debug(\n            f\"Time took to execute the function {func} \\\n            with args {args}, kwargs {kwargs} is {time_elapsed} seconds\"\n        )\n        return ret_val\n\n    return wrapper\n</code></pre>"},{"location":"utils_util/#erad.utils.util.write_file","title":"<code>write_file(content, file_path, **kwargs)</code>","text":"<p>Utility function to write to a file..</p> <p>Supports json, yaml and geojson.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>dict</code> <p>Python dict content</p> required <code>file_path</code> <code>str</code> <p>Path to a file to be read</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments passed to relevant writer.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FeatureNotImplementedError</code> <p>Raises if invalid file type is passed.</p> Source code in <code>erad\\utils\\util.py</code> <pre><code>def write_file(content: dict, file_path: str, **kwargs) -&gt; None:\n\"\"\"Utility function to write to a file..\n\n    Supports json, yaml and geojson.\n\n    Args:\n        content (dict): Python dict content\n        file_path (str): Path to a file to be read\n        kwargs (dict): Keyword arguments passed to\n            relevant writer.\n\n    Raises:\n        FeatureNotImplementedError: Raises if invalid file type is passed.\n    \"\"\"\n    file_path = Path(file_path)\n    path_validation(file_path.parent)\n\n    # Handle JSON file write\n    if file_path.suffix == \".json\":\n        with open(file_path, \"w\") as f:\n            json.dump(content, f, **kwargs)\n\n    # Handle YAML file write\n    elif file_path.suffix == \".yaml\":\n        with open(file_path, \"w\") as f:\n            yaml.safe_dump(content, f, **kwargs)\n\n    # Handle geojson file write\n    elif file_path.suffix == \".geojson\":\n        with open(file_path, \"w\") as f:\n            geojson.dump(content, f, **kwargs)\n\n    else:\n        raise FeatureNotImplementedError(\n            f\"File of type {file_path.suffix} \\\n            is not yet implemented for writing purpose\"\n        )\n</code></pre>"},{"location":"visualization_plot_graph/","title":"visualization.plot_graph","text":"<p>Module for handling graph plots.</p>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.AbstractGraphPlot","title":"<code>AbstractGraphPlot</code>","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Abstract interface for developing subclass to plot network graph.</p> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>class AbstractGraphPlot(abc.ABC):\n\"\"\"Abstract interface for developing subclass to plot network graph.\"\"\"\n\n    @abc.abstractmethod\n    def add_network_data(self, *args, **kwargs):\n\"\"\"Abstract method for adding network data.\"\"\"\n\n    @abc.abstractmethod\n    def prepare_plot(self, *args, **kwargs):\n\"\"\"Abstract method for preparing and showing teh plot\"\"\"\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.AbstractGraphPlot.add_network_data","title":"<code>add_network_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for adding network data.</p> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>@abc.abstractmethod\ndef add_network_data(self, *args, **kwargs):\n\"\"\"Abstract method for adding network data.\"\"\"\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.AbstractGraphPlot.prepare_plot","title":"<code>prepare_plot(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for preparing and showing teh plot</p> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>@abc.abstractmethod\ndef prepare_plot(self, *args, **kwargs):\n\"\"\"Abstract method for preparing and showing teh plot\"\"\"\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph","title":"<code>PloltyGraph</code>","text":"<p>         Bases: <code>AbstractGraphPlot</code></p> <p>Class for managing graph plot using Plotly.</p> <p>Attributes:</p> Name Type Description <code>access_token</code> <code>str</code> <p>MapBox API token</p> <code>style</code> <code>str</code> <p>MapBox style</p> <code>zoom_level</code> <code>int</code> <p>Zoom level for the plot</p> <code>data</code> <code>List</code> <p>Stores the data to be fed to plotly for plotting</p> <code>scatter_data</code> <code>Dict</code> <p>Stores longitudes and latitudes of nodes from network</p> <code>fig</code> <code>go.Figure</code> <p>Plotly graph objects figure instance</p> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>class PloltyGraph(AbstractGraphPlot):\n\"\"\"Class for managing graph plot using Plotly.\n\n    Attributes:\n        access_token (str): MapBox API token\n        style (str): MapBox style\n        zoom_level (int): Zoom level for the plot\n        data (List): Stores the data to be fed to plotly for plotting\n        scatter_data (Dict): Stores longitudes and latitudes of nodes\n            from network\n        fig (go.Figure): Plotly graph objects figure instance\n    \"\"\"\n\n    def __init__(\n        self,\n        access_token: str = None,\n        style: str = \"carto-darkmatter\",\n        zoom_level: int = 13,\n    ) -&gt; None:\n\"\"\"Constructor for `PlotlyGraph` Subclass.\n\n        Args:\n            access_token (str): MapBox API token\n            style (str): MapBox style\n            zoom_level (int): Zoom level for the plot\n        \"\"\"\n\n        if access_token:\n            self.access_token = access_token\n        else:\n            self.access_token = os.getenv(\"MAPBOX_API_KEY\")\n        self.style = style\n        self.zoom_level = zoom_level\n\n        self.data = []\n\n    def _get_map_centre(self, longitudes: List[float], latitudes: List[float]):\n\"\"\"Returns map center.\"\"\"\n        return {\n            \"lon\": sum(longitudes) / len(longitudes),\n            \"lat\": sum(latitudes) / len(latitudes),\n        }\n\n    def add_network_data(\n        self,\n        network: nx.Graph,\n        latitude_property: str = \"lat\",\n        longitude_property: str = \"long\",\n        node_color: str = \"blue\",\n        line_color: str = \"red\",\n    ) -&gt; None:\n\"\"\"Method to add network data to plot data.\n\n        Args:\n            network (nx.Graph): Networkx graph instance\n            latitude_property (str): Property name to be\n                used as latitude\n            longitude_property (str): Property name to be\n                used as longitude\n            node_color (str): Color name to be used to plot\n                nodes\n            line_color (str): Color name to be used to plot\n                line segments\n        \"\"\"\n\n        # Add nodes\n        self.scatter_data = {\"latitudes\": [], \"longitudes\": []}\n\n        for node in network.nodes.data():\n\n            # Storing the lat lons in scatter data\n            # container\n            self.scatter_data[\"latitudes\"].append(node[1][latitude_property])\n            self.scatter_data[\"longitudes\"].append(node[1][longitude_property])\n\n        # Stroing the edge data in container\n        line_data = {\"latitudes\": [], \"longitudes\": []}\n        node_data = {node[0]: node[1] for node in network.nodes.data()}\n\n        for edge in network.edges():\n            line_data[\"latitudes\"].extend(\n                [\n                    node_data[edge[0]][latitude_property],\n                    node_data[edge[1]][latitude_property],\n                    None,\n                ]\n            )\n\n            line_data[\"longitudes\"].extend(\n                [\n                    node_data[edge[0]][longitude_property],\n                    node_data[edge[1]][longitude_property],\n                    None,\n                ]\n            )\n\n        # Adding plots to plotly graph object\n        self.data.append(\n            go.Scattermapbox(\n                mode=\"markers\",\n                lon=self.scatter_data[\"longitudes\"],\n                lat=self.scatter_data[\"latitudes\"],\n                marker={\"size\": 5, \"color\": node_color},\n            )\n        )\n\n        self.data.append(\n            go.Scattermapbox(\n                mode=\"markers+lines\",\n                lon=line_data[\"longitudes\"],\n                lat=line_data[\"latitudes\"],\n                marker={\"size\": 0},\n                line={\"color\": line_color},\n            )\n        )\n\n    def add_scatter_points(\n        self,\n        latitudes: List[float],\n        longitudes: List[float],\n        color: str = \"yellow\",\n        size: int = 5,\n    ) -&gt; None:\n\"\"\"Method for scatter points to plot data.\n\n        Args:\n            latitudes (List[float]): List of latitude points\n            longitudes (List[float]): List of longitude points\n            color (str): Color to be used for scatter points\n            size (int): Size of scatter points\n        \"\"\"\n\n        self.data.append(\n            go.Scattermapbox(\n                mode=\"markers\",\n                lon=longitudes,\n                lat=latitudes,\n                marker={\"size\": size, \"color\": color},\n            )\n        )\n\n    def add_polygon(\n        self,\n        latitudes: List[float],\n        longitudes: List[float],\n        fill: str = \"toself\",\n    ) -&gt; None:\n\"\"\"Method for adding polygon to the plot.\n\n        Args:\n            latitudes (List[float]): List of latitude points\n            longitudes (List[float]): List of longitude points\n            fill (str): Accepted fill value by plotly\n        \"\"\"\n        self.data.append(\n            go.Scattermapbox(\n                lon=longitudes, lat=latitudes, fill=fill, mode=\"lines\"\n            )\n        )\n\n    def prepare_plot(self, show: bool = True):\n\"\"\"Method to prepare and show the plot.\n\n        Args:\n            show (bool): True if want to see the plot.\n        \"\"\"\n        self.fig = go.Figure(data=self.data)\n        self.fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n        self.fig.update_mapboxes(\n            {\n                \"accesstoken\": self.access_token,\n                \"style\": self.style,\n                \"center\": self._get_map_centre(\n                    self.scatter_data[\"longitudes\"],\n                    self.scatter_data[\"latitudes\"],\n                ),\n                \"zoom\": self.zoom_level,\n            }\n        )\n\n        if show:\n            self.fig.show()\n\n    def html_export(self, html_file_path: str):\n\"\"\"Method for exporting plot as HTML file.\"\"\"\n        path_validation(html_file_path)\n        self.fig.write_html(html_file_path)\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.__init__","title":"<code>__init__(access_token=None, style='carto-darkmatter', zoom_level=13)</code>","text":"<p>Constructor for <code>PlotlyGraph</code> Subclass.</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>MapBox API token</p> <code>None</code> <code>style</code> <code>str</code> <p>MapBox style</p> <code>'carto-darkmatter'</code> <code>zoom_level</code> <code>int</code> <p>Zoom level for the plot</p> <code>13</code> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def __init__(\n    self,\n    access_token: str = None,\n    style: str = \"carto-darkmatter\",\n    zoom_level: int = 13,\n) -&gt; None:\n\"\"\"Constructor for `PlotlyGraph` Subclass.\n\n    Args:\n        access_token (str): MapBox API token\n        style (str): MapBox style\n        zoom_level (int): Zoom level for the plot\n    \"\"\"\n\n    if access_token:\n        self.access_token = access_token\n    else:\n        self.access_token = os.getenv(\"MAPBOX_API_KEY\")\n    self.style = style\n    self.zoom_level = zoom_level\n\n    self.data = []\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.add_network_data","title":"<code>add_network_data(network, latitude_property='lat', longitude_property='long', node_color='blue', line_color='red')</code>","text":"<p>Method to add network data to plot data.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>nx.Graph</code> <p>Networkx graph instance</p> required <code>latitude_property</code> <code>str</code> <p>Property name to be used as latitude</p> <code>'lat'</code> <code>longitude_property</code> <code>str</code> <p>Property name to be used as longitude</p> <code>'long'</code> <code>node_color</code> <code>str</code> <p>Color name to be used to plot nodes</p> <code>'blue'</code> <code>line_color</code> <code>str</code> <p>Color name to be used to plot line segments</p> <code>'red'</code> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def add_network_data(\n    self,\n    network: nx.Graph,\n    latitude_property: str = \"lat\",\n    longitude_property: str = \"long\",\n    node_color: str = \"blue\",\n    line_color: str = \"red\",\n) -&gt; None:\n\"\"\"Method to add network data to plot data.\n\n    Args:\n        network (nx.Graph): Networkx graph instance\n        latitude_property (str): Property name to be\n            used as latitude\n        longitude_property (str): Property name to be\n            used as longitude\n        node_color (str): Color name to be used to plot\n            nodes\n        line_color (str): Color name to be used to plot\n            line segments\n    \"\"\"\n\n    # Add nodes\n    self.scatter_data = {\"latitudes\": [], \"longitudes\": []}\n\n    for node in network.nodes.data():\n\n        # Storing the lat lons in scatter data\n        # container\n        self.scatter_data[\"latitudes\"].append(node[1][latitude_property])\n        self.scatter_data[\"longitudes\"].append(node[1][longitude_property])\n\n    # Stroing the edge data in container\n    line_data = {\"latitudes\": [], \"longitudes\": []}\n    node_data = {node[0]: node[1] for node in network.nodes.data()}\n\n    for edge in network.edges():\n        line_data[\"latitudes\"].extend(\n            [\n                node_data[edge[0]][latitude_property],\n                node_data[edge[1]][latitude_property],\n                None,\n            ]\n        )\n\n        line_data[\"longitudes\"].extend(\n            [\n                node_data[edge[0]][longitude_property],\n                node_data[edge[1]][longitude_property],\n                None,\n            ]\n        )\n\n    # Adding plots to plotly graph object\n    self.data.append(\n        go.Scattermapbox(\n            mode=\"markers\",\n            lon=self.scatter_data[\"longitudes\"],\n            lat=self.scatter_data[\"latitudes\"],\n            marker={\"size\": 5, \"color\": node_color},\n        )\n    )\n\n    self.data.append(\n        go.Scattermapbox(\n            mode=\"markers+lines\",\n            lon=line_data[\"longitudes\"],\n            lat=line_data[\"latitudes\"],\n            marker={\"size\": 0},\n            line={\"color\": line_color},\n        )\n    )\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.add_polygon","title":"<code>add_polygon(latitudes, longitudes, fill='toself')</code>","text":"<p>Method for adding polygon to the plot.</p> <p>Parameters:</p> Name Type Description Default <code>latitudes</code> <code>List[float]</code> <p>List of latitude points</p> required <code>longitudes</code> <code>List[float]</code> <p>List of longitude points</p> required <code>fill</code> <code>str</code> <p>Accepted fill value by plotly</p> <code>'toself'</code> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def add_polygon(\n    self,\n    latitudes: List[float],\n    longitudes: List[float],\n    fill: str = \"toself\",\n) -&gt; None:\n\"\"\"Method for adding polygon to the plot.\n\n    Args:\n        latitudes (List[float]): List of latitude points\n        longitudes (List[float]): List of longitude points\n        fill (str): Accepted fill value by plotly\n    \"\"\"\n    self.data.append(\n        go.Scattermapbox(\n            lon=longitudes, lat=latitudes, fill=fill, mode=\"lines\"\n        )\n    )\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.add_scatter_points","title":"<code>add_scatter_points(latitudes, longitudes, color='yellow', size=5)</code>","text":"<p>Method for scatter points to plot data.</p> <p>Parameters:</p> Name Type Description Default <code>latitudes</code> <code>List[float]</code> <p>List of latitude points</p> required <code>longitudes</code> <code>List[float]</code> <p>List of longitude points</p> required <code>color</code> <code>str</code> <p>Color to be used for scatter points</p> <code>'yellow'</code> <code>size</code> <code>int</code> <p>Size of scatter points</p> <code>5</code> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def add_scatter_points(\n    self,\n    latitudes: List[float],\n    longitudes: List[float],\n    color: str = \"yellow\",\n    size: int = 5,\n) -&gt; None:\n\"\"\"Method for scatter points to plot data.\n\n    Args:\n        latitudes (List[float]): List of latitude points\n        longitudes (List[float]): List of longitude points\n        color (str): Color to be used for scatter points\n        size (int): Size of scatter points\n    \"\"\"\n\n    self.data.append(\n        go.Scattermapbox(\n            mode=\"markers\",\n            lon=longitudes,\n            lat=latitudes,\n            marker={\"size\": size, \"color\": color},\n        )\n    )\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.html_export","title":"<code>html_export(html_file_path)</code>","text":"<p>Method for exporting plot as HTML file.</p> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def html_export(self, html_file_path: str):\n\"\"\"Method for exporting plot as HTML file.\"\"\"\n    path_validation(html_file_path)\n    self.fig.write_html(html_file_path)\n</code></pre>"},{"location":"visualization_plot_graph/#erad.visualization.plot_graph.PloltyGraph.prepare_plot","title":"<code>prepare_plot(show=True)</code>","text":"<p>Method to prepare and show the plot.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>True if want to see the plot.</p> <code>True</code> Source code in <code>erad\\visualization\\plot_graph.py</code> <pre><code>def prepare_plot(self, show: bool = True):\n\"\"\"Method to prepare and show the plot.\n\n    Args:\n        show (bool): True if want to see the plot.\n    \"\"\"\n    self.fig = go.Figure(data=self.data)\n    self.fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n    self.fig.update_mapboxes(\n        {\n            \"accesstoken\": self.access_token,\n            \"style\": self.style,\n            \"center\": self._get_map_centre(\n                self.scatter_data[\"longitudes\"],\n                self.scatter_data[\"latitudes\"],\n            ),\n            \"zoom\": self.zoom_level,\n        }\n    )\n\n    if show:\n        self.fig.show()\n</code></pre>"}]}